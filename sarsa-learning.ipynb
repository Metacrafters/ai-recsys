{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c3990e",
   "metadata": {},
   "source": [
    "Below is a complete SARSA tutorial written in markdown. It starts with an explanation of the SARSA algorithm in pseudocode (verbal description) and then shows a detailed Python example using a simple maze problem. The SARSA update equation and all key components are explained.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. SARSA Algorithm Overview\n",
    "\n",
    "**SARSA** stands for State-Action-Reward-State-Action, and it’s an on-policy Temporal-Difference (TD) control method. The key difference from Q-learning is that SARSA updates its Q-values using the actual action taken in the next state (following the current policy), whereas Q-learning uses the maximum estimated value for the next state.\n",
    "\n",
    "### SARSA Pseudocode (Verbal Description)\n",
    "\n",
    "1. **Initialize** all Q(s, a) arbitrarily for all state-action pairs.\n",
    "2. For each **episode** (from start to termination):\n",
    "   - Set initial state, s.\n",
    "   - Choose an initial action, a, using an *epsilon-greedy* policy based on Q.\n",
    "   - While s is not terminal:\n",
    "     - Execute action a, observe reward r and next state s′.\n",
    "     - Choose the next action a′ in s′ following the same policy (epsilon-greedy).\n",
    "     - **Update** Q(s, a) using the SARSA equation:  \n",
    "       Q(s, a) ← Q(s, a) + α [r + γ · Q(s′, a′) − Q(s, a)]  \n",
    "         - Here, α is the learning rate.\n",
    "         - γ is the discount factor.\n",
    "     - Set (s, a) ← (s′, a′).\n",
    "\n",
    "3. Repeat for many episodes so that Q(s, a) approximates the optimal action-value function while following the current policy.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Detailed Python Tutorial Using a Simple Maze Problem\n",
    "\n",
    "In this example, we use a maze environment where:\n",
    "- The maze is a 2D grid defined as a NumPy array.\n",
    "- 0 represents a free cell, and 1 represents a wall.\n",
    "- The agent’s actions are: up, down, left, right.\n",
    "- The agent receives a reward of –1 for every step and +100 when the goal is reached.\n",
    "- If the agent attempts to move into a wall or outside the maze, it remains in the same cell.\n",
    "\n",
    "Below is the Python source code implementing the SARSA algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3a83eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 completed.\n",
      "Episode 200 completed.\n",
      "Episode 300 completed.\n",
      "Episode 400 completed.\n",
      "Episode 500 completed.\n",
      "\n",
      "Learned Q-table:\n",
      "[[  8.53590385  15.81467581   8.38920066   3.78890365]\n",
      " [ -3.90946426  -3.88820822   9.80354611  -3.50214467]\n",
      " [ -3.45527943  -1.78592367  -3.46122563  -3.35284734]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -0.97647478  -1.04523739  -1.02292406  -0.96721852]\n",
      " [ -0.95935281  -0.91677845  -1.00773746  -0.99235258]\n",
      " [  7.73899746  22.43052139  12.14505378  11.82572409]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -2.85996684   1.01707719  -2.81547598  -2.78406192]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -1.01270887  -0.54262808  -1.08980124  -1.10441679]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 13.22927893  29.13458281  17.4641735   16.64728312]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -2.28728335  -1.4827975   -2.00583841   7.20109631]\n",
      " [ -1.6902753   -0.76220184  -1.39156456  18.75493422]\n",
      " [ -1.10491751   1.31298203  -0.84629953  35.3443596 ]\n",
      " [  7.18660679  48.33914096  -0.54938957   4.13299585]\n",
      " [ 15.32418365  20.4676509   23.46850099  34.49857535]\n",
      " [ 23.91659122  45.38985604  22.32594225  16.45366758]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  4.5659821   73.7657484    9.57898392  14.58540643]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 26.20582813  16.68369308  34.36389105  52.48996306]\n",
      " [ 43.20033105  33.86466647  34.41264928  59.95177341]\n",
      " [ 52.24873279  70.51997003  37.77635777  57.37490376]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  6.95690034  99.42735831  16.67925687  22.77743448]\n",
      " [ -1.58868831  -1.28570628  -1.62929332   2.21786213]\n",
      " [ 38.62640689   6.01165222  -1.602208     2.29248117]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 56.33569082  64.26392871  61.19659068  87.06327545]\n",
      " [ 73.93712297  80.32558744  58.65145527 100.        ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "\n",
      "Learned Policy (arrows indicate best action):\n",
      "↓ ← ↓ █ → ↓\n",
      "↓ █ ↓ █ ↓ █\n",
      "↓ █ → → → ↓\n",
      "→ ↓ █ █ █ ↓\n",
      "█ → → ↓ █ ↓\n",
      "→ ↑ █ → → G\n",
      "\n",
      "Test Run:\n",
      "Path taken: [(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (4, 1), (4, 2), (4, 3), (5, 3), (5, 4), (5, 5)]\n",
      "Goal reached in 10 steps!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ---------------------------\n",
    "# Maze Environment Definition\n",
    "# ---------------------------\n",
    "class MazeEnv:\n",
    "    def __init__(self):\n",
    "        # Define the maze layout: 0 = free space, 1 = wall.\n",
    "        # Example: a 6x6 maze.\n",
    "        self.maze = np.array([\n",
    "            [0, 0, 0, 1, 0, 0],\n",
    "            [0, 1, 0, 1, 0, 1],\n",
    "            [0, 1, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 1, 1, 0],\n",
    "            [1, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 1, 0, 0, 0]\n",
    "        ])\n",
    "        self.n_rows, self.n_cols = self.maze.shape\n",
    "        \n",
    "        # Define start and goal positions.\n",
    "        self.start_state = (0, 0)   # Top-left corner.\n",
    "        self.goal_state = (5, 5)    # Bottom-right corner.\n",
    "        \n",
    "        # Define available actions: up, down, left, right.\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),   # Up\n",
    "            1: (1, 0),    # Down\n",
    "            2: (0, -1),   # Left\n",
    "            3: (0, 1)     # Right\n",
    "        }\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take one step in the maze.\n",
    "        Parameters: action (0: up, 1: down, 2: left, 3: right)\n",
    "        Returns: next_state, reward, done (True if goal is reached)\n",
    "        \"\"\"\n",
    "        # Get the movement delta for the action.\n",
    "        delta = self.action_space[action]\n",
    "        next_state = (self.current_state[0] + delta[0],\n",
    "                      self.current_state[1] + delta[1])\n",
    "        \n",
    "        # Boundary check: if out of bounds, remain in current state.\n",
    "        if (next_state[0] < 0 or next_state[0] >= self.n_rows or\n",
    "            next_state[1] < 0 or next_state[1] >= self.n_cols):\n",
    "            next_state = self.current_state\n",
    "        # Check if hitting a wall\n",
    "        elif self.maze[next_state] == 1:\n",
    "            next_state = self.current_state\n",
    "        \n",
    "        # Define reward: +100 if goal reached, -1 for each step.\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Convert a (row, col) state into an index for Q table.\"\"\"\n",
    "        return state[0] * self.n_cols + state[1]\n",
    "\n",
    "    def print_policy(self, Q):\n",
    "        \"\"\"Print the learned policy as arrows for each cell (except walls).\"\"\"\n",
    "        directions = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
    "        policy = np.full(self.maze.shape, ' ')\n",
    "        for r in range(self.n_rows):\n",
    "            for c in range(self.n_cols):\n",
    "                if self.maze[r, c] == 1:\n",
    "                    policy[r, c] = '█'\n",
    "                elif (r, c) == self.goal_state:\n",
    "                    policy[r, c] = 'G'\n",
    "                else:\n",
    "                    state_index = self.state_to_index((r, c))\n",
    "                    best_action = np.argmax(Q[state_index])\n",
    "                    policy[r, c] = directions[best_action]\n",
    "        for row in policy:\n",
    "            print(' '.join(row))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# SARSA Algorithm Parameters\n",
    "# ---------------------------\n",
    "alpha = 0.1         # Learning rate\n",
    "gamma = 0.9         # Discount factor\n",
    "epsilon = 0.2       # Exploration probability for epsilon-greedy\n",
    "num_episodes = 500  # Number of episodes for training\n",
    "\n",
    "# Initialize the environment and Q table.\n",
    "env = MazeEnv()\n",
    "n_states = env.n_rows * env.n_cols\n",
    "n_actions = env.n_actions\n",
    "Q = np.zeros((n_states, n_actions))  # Q table with dimensions: states x actions\n",
    "\n",
    "# SARSA update equation:\n",
    "# Q(s, a) ← Q(s, a) + α * [r + γ * Q(s′, a′) − Q(s, a)]\n",
    "\n",
    "# ---------------------------\n",
    "# SARSA Training Loop\n",
    "# ---------------------------\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state_index = env.state_to_index(state)\n",
    "    \n",
    "    # Choose initial action using an epsilon-greedy policy.\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = random.choice(list(env.action_space.keys()))\n",
    "    else:\n",
    "        action = np.argmax(Q[state_index])\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Take the action, observe reward and next state.\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state_index = env.state_to_index(next_state)\n",
    "        \n",
    "        # Choose next action based on the current policy (epsilon-greedy).\n",
    "        if not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                next_action = random.choice(list(env.action_space.keys()))\n",
    "            else:\n",
    "                next_action = np.argmax(Q[next_state_index])\n",
    "        else:\n",
    "            # If episode is finished, there is no next action.\n",
    "            next_action = None\n",
    "        \n",
    "        # SARSA update.\n",
    "        # If next_action is None (terminal state), Q(next_state, next_action) is 0.\n",
    "        q_current = Q[state_index, action]\n",
    "        if next_action is not None:\n",
    "            q_target = reward + gamma * Q[next_state_index, next_action]\n",
    "        else:\n",
    "            q_target = reward  # For terminal state\n",
    "            \n",
    "        Q[state_index, action] = q_current + alpha * (q_target - q_current)\n",
    "        \n",
    "        # Move to the next state and action.\n",
    "        state_index = next_state_index\n",
    "        action = next_action if next_action is not None else action\n",
    "\n",
    "    # Optionally print progress every 100 episodes.\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(\"Episode {} completed.\".format(episode + 1))\n",
    "\n",
    "# ---------------------------\n",
    "# Display the Results\n",
    "# ---------------------------\n",
    "print(\"\\nLearned Q-table:\")\n",
    "print(Q)\n",
    "\n",
    "print(\"\\nLearned Policy (arrows indicate best action):\")\n",
    "env.print_policy(Q)\n",
    "\n",
    "# ---------------------------\n",
    "# Test the Learned Policy\n",
    "# ---------------------------\n",
    "print(\"\\nTest Run:\")\n",
    "state = env.reset()\n",
    "steps = 0\n",
    "path = [state]\n",
    "done = False\n",
    "\n",
    "while not done and steps < 50:\n",
    "    state_index = env.state_to_index(state)\n",
    "    action = np.argmax(Q[state_index])\n",
    "    state, reward, done = env.step(action)\n",
    "    path.append(state)\n",
    "    steps += 1\n",
    "\n",
    "print(\"Path taken:\", path)\n",
    "if done:\n",
    "    print(\"Goal reached in {} steps!\".format(steps))\n",
    "else:\n",
    "    print(\"Failed to reach goal within step limit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1aa1d8",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "### Environment (MazeEnv)\n",
    "- **Maze Setup**: The maze is defined using a NumPy array where:\n",
    "  - 0 represents an open cell.\n",
    "  - 1 represents a wall.\n",
    "- **Actions**: The agent can move in four directions (up, down, left, right). Moves that hit a wall or go out of bounds leave the agent in the same state.\n",
    "- **Rewards**: The agent gets –1 per step and a reward of +100 on reaching the goal.\n",
    "- **Helper Methods**:\n",
    "  - `reset()`: Initializes the environment.\n",
    "  - `step(action)`: Executes an action and returns the next state, reward, and a boolean indicating if the goal is reached.\n",
    "  - `state_to_index(state)`: Converts the (row, col) state into a unique index (used to index the Q table).\n",
    "  - `print_policy(Q)`: Displays the learned policy, marking the best action from the Q table with arrows.\n",
    "\n",
    "### SARSA Training Loop\n",
    "- **Initialization**: The Q table is initialized with zeros for all state-action pairs.\n",
    "- **Epsilon-Greedy Policy**: Both the initial action and the next actions are chosen using an epsilon-greedy strategy, which allows exploration.\n",
    "- **SARSA Update**:  \n",
    "  The Q-value is updated with the following equation:  \n",
    "  Q(s, a) ← Q(s, a) + α · [r + γ · Q(s′, a′) − Q(s, a)]  \n",
    "  Here:\n",
    "  - s, a: current state and action.\n",
    "  - r: reward received after taking action a.\n",
    "  - s′, a′: next state and the action chosen in that next state.\n",
    "  - α: learning rate.\n",
    "  - γ: discount factor.\n",
    "- **Episode Termination**: If the goal is reached, the episode ends.\n",
    "\n",
    "### Testing the Learned Policy\n",
    "- After training, the code runs a test episode where the agent always picks the action with the highest Q-value to demonstrate the learned path from the start to the goal.\n",
    "\n",
    "---\n",
    "# Difference between Q-learning and SARSA\n",
    "The key difference between Q-learning and SARSA is that Q-learning is an off-policy algorithm while SARSA is an on-policy algorithm.\n",
    "\n",
    "- In **Q-learning**, the update rule uses the maximum estimated Q-value for the next state (i.e., it assumes the best possible action will be taken next). This means that even if the agent follows an exploratory (non-greedy) behavior, the update still uses a greedy estimate:  \n",
    "\n",
    "  Q(s, a) ← Q(s, a) + α [r + γ · **maxₐ′ Q(s′, a′)** − Q(s, a)]\n",
    "\n",
    "- In **SARSA**, the update rule uses the actual action that the agent takes in the next state according to its current (often exploratory) policy. This means that the learning is affected by the current policy’s propensity for exploration:  \n",
    "\n",
    "  Q(s, a) ← Q(s, a) + α [r + γ · **Q(s′, a′)** − Q(s, a)]\n",
    "\n",
    "In summary, Q-learning is focused on learning the optimal policy regardless of the behavior policy (off-policy), while SARSA learns the value of following the current policy (on-policy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dec-finetuning-dev-gfUsYha8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
