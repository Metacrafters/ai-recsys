{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be72dbf-c2c0-47cf-aaac-3ea5e4256db8",
   "metadata": {},
   "source": [
    "# Q-learning Tutorial using Python\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Summarize the Q-learning algorithm using a pseudocode (verbal) description.\n",
    "2. Implement a detailed example using a simple maze problem.\n",
    "\n",
    "The maze is a grid where:\n",
    "  - 0 represents a free cell the agent can move into,\n",
    "  - 1 represents a wall (obstacle), and\n",
    "  - The agent receives a reward of -1 for each move and +100 at the goal.\n",
    "\n",
    "The Q-learning update rule is:\n",
    "\n",
    "  Q(s, a) = Q(s, a) + α * [r + γ * max_a′Q(s′,a′) - Q(s, a)]\n",
    "\n",
    "where:\n",
    "  - α (alpha) is the learning rate,\n",
    "  - γ (gamma) is the discount factor,\n",
    "  - r is the reward, and\n",
    "  - s and s′ are the current and next states respectively.\n",
    "\n",
    "**Links**\n",
    "- https://medium.com/@alwinraju/in-depth-guide-to-implementing-q-learning-in-python-with-openai-gyms-taxi-environment-cd356cc6a288\n",
    "- https://github.com/vmayoral/basic_reinforcement_learning/blob/master/tutorial1/README.md\n",
    "- https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "- https://en.wikipedia.org/wiki/Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31269434-ef87-4d39-a007-d65401f70a08",
   "metadata": {},
   "source": [
    "## Pseudocode Summary for Q-learning\n",
    "\n",
    "1. Initialize Q(s, a) arbitrarily for all state–action pairs.\n",
    "2. For each episode:\n",
    "    - Reset the environment starting from the initial state.\n",
    "    - While the state is not terminal:\n",
    "        1. Choose an action a using an epsilon-greedy policy.\n",
    "        2. Execute action a, obtain reward r and new state s′.\n",
    "        3. Update Q(s, a) as follows:\n",
    "          \n",
    "             Q(s, a) ← Q(s, a) + α * [r + γ * maxₐ′ Q(s′, a′) - Q(s, a)]\n",
    "        4. Set s ← s′.\n",
    "3. Repeat until the policy converges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f125d95-0f9c-4cd2-af86-758509debb29",
   "metadata": {},
   "source": [
    "## Maze Environment and Q-learning Implementation\n",
    "\n",
    "In the next code cell we define the maze environment as a Python class and implement the complete Q-learning algorithm. The maze is represented by a 2D NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a05080-787e-41c6-997a-514ed625af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 completed.\n",
      "Episode 200 completed.\n",
      "Episode 300 completed.\n",
      "Episode 400 completed.\n",
      "Episode 500 completed.\n",
      "\n",
      "Learned Q-table:\n",
      "[[ 28.34020005  24.47722978  28.35371127  32.61625379]\n",
      " [ 32.60586859  32.61243338  28.3476529   37.3513931 ]\n",
      " [ 37.34479665  42.612659    32.61054185  37.34804752]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  2.72204597  46.75775064   6.10292696  -0.98781656]\n",
      " [ -0.99673864  -1.09293229   1.40205811  -0.94862952]\n",
      " [ 28.34815535   8.35215522  17.3825928   20.38908024]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 37.34747087  48.45851     42.60868331  42.61175886]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 26.44559286  62.17041029  46.11355035  41.93833196]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -0.14429012  17.19331759   0.3455677    1.23546592]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 42.60963061  48.45566475  48.45687693  54.9539    ]\n",
      " [ 54.95140903  54.95252071  48.45728793  62.171     ]\n",
      " [ 54.9470256   62.17088458  54.94851508  70.19      ]\n",
      " [ 70.18828078  79.1         62.17025438  70.1837827 ]\n",
      " [ -0.61940364   7.58484054   2.62298297  27.92413127]\n",
      " [ 14.67709843  41.44791441   7.96719422  10.29895712]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 70.18965849  89.          79.09571059  79.0981926 ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 13.6021093    5.38421937  16.55668684  53.48769019]\n",
      " [ 26.30870606  33.16639013  24.8027755   65.20428602]\n",
      " [ 30.05214519  77.2844814   26.20778478  39.73179976]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 79.09255854 100.          88.98321156  88.99304312]\n",
      " [ -1.45788998  -1.48371978  -1.51451152  -0.15279121]\n",
      " [ 23.00699044   0.38181265  -1.57903171   1.44137183]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ 34.4935796   52.47843218  51.83162034  88.40260565]\n",
      " [ 46.81064583  45.18881876  54.08873833  99.85444217]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "\n",
      "Learned Policy (arrows show the best action for each cell):\n",
      "→ → ↓ █ ↓ ←\n",
      "↑ █ ↓ █ ↓ █\n",
      "↓ █ → → → ↓\n",
      "→ ↓ █ █ █ ↓\n",
      "█ → → ↓ █ ↓\n",
      "→ ↑ █ → → G\n",
      "\n",
      "Test Run:\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (2, 3), (2, 4), (2, 5), (3, 5), (4, 5), (5, 5)]\n",
      "Goal reached in 10 steps!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class MazeEnv:\n",
    "    def __init__(self):\n",
    "        # Maze layout: 0 = free space, 1 = wall\n",
    "        # A sample 6x6 maze\n",
    "        self.maze = np.array(\n",
    "            [\n",
    "                [0, 0, 0, 1, 0, 0],\n",
    "                [0, 1, 0, 1, 0, 1],\n",
    "                [0, 1, 0, 0, 0, 0],\n",
    "                [0, 0, 1, 1, 1, 0],\n",
    "                [1, 0, 0, 0, 1, 0],\n",
    "                [0, 0, 1, 0, 0, 0],\n",
    "            ]\n",
    "        )\n",
    "        self.n_rows, self.n_cols = self.maze.shape\n",
    "\n",
    "        # Define start and goal positions\n",
    "        self.start_state = (0, 0)  # Top-left corner\n",
    "        self.goal_state = (5, 5)  # Bottom-right corner\n",
    "\n",
    "        # Define actions: up, down, left, right\n",
    "        self.action_space = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),  # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1),  # Right\n",
    "        }\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the maze environment.\n",
    "        Input:\n",
    "            action (int): 0 (up), 1 (down), 2 (left), 3 (right)\n",
    "        Returns:\n",
    "            next_state (tuple): the new state after taking the action\n",
    "            reward (int): reward for the action\n",
    "            done (bool): True if the goal is reached\n",
    "        \"\"\"\n",
    "        delta = self.action_space[action]\n",
    "        next_state = (\n",
    "            self.current_state[0] + delta[0],\n",
    "            self.current_state[1] + delta[1],\n",
    "        )\n",
    "\n",
    "        # Check boundaries\n",
    "        if (\n",
    "            next_state[0] < 0\n",
    "            or next_state[0] >= self.n_rows\n",
    "            or next_state[1] < 0\n",
    "            or next_state[1] >= self.n_cols\n",
    "        ):\n",
    "            next_state = self.current_state\n",
    "        # Check if the next state is a wall\n",
    "        elif self.maze[next_state] == 1:\n",
    "            next_state = self.current_state\n",
    "\n",
    "        # Define reward and termination condition\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Convert a (row, col) state into a flat index.\"\"\"\n",
    "        return state[0] * self.n_cols + state[1]\n",
    "\n",
    "    def print_policy(self, Q):\n",
    "        \"\"\"Display the learned policy with arrows for each action.\"\"\"\n",
    "        directions = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
    "        policy = np.full(self.maze.shape, \" \")\n",
    "        for r in range(self.n_rows):\n",
    "            for c in range(self.n_cols):\n",
    "                if self.maze[r, c] == 1:\n",
    "                    policy[r, c] = \"█\"\n",
    "                elif (r, c) == self.goal_state:\n",
    "                    policy[r, c] = \"G\"\n",
    "                else:\n",
    "                    state_index = self.state_to_index((r, c))\n",
    "                    best_action = np.argmax(Q[state_index])\n",
    "                    policy[r, c] = directions[best_action]\n",
    "        for row in policy:\n",
    "            print(\" \".join(row))\n",
    "\n",
    "\n",
    "# Q-learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.5  # Exploration probability\n",
    "num_episodes = 500  # Number of training episodes\n",
    "\n",
    "# Initialize environment and Q-table\n",
    "env = MazeEnv()\n",
    "n_states = env.n_rows * env.n_cols\n",
    "n_actions = env.n_actions\n",
    "\n",
    "# Q-table initialization: each row for a state, each column for an action\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Q-learning Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state_index = env.state_to_index(state)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose action with epsilon-greedy policy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(list(env.action_space.keys()))\n",
    "        else:\n",
    "            action = np.argmax(Q[state_index])\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state_index = env.state_to_index(next_state)\n",
    "\n",
    "        # Q-learning update equation:\n",
    "        # Q(s, a) = Q(s, a) + alpha * [reward + gamma * max_a' Q(s', a') - Q(s, a)]\n",
    "        curr_q = Q[state_index, action]\n",
    "        target = reward + gamma * np.max(Q[next_state_index])\n",
    "        Q[state_index, action] = curr_q + alpha * (target - curr_q)\n",
    "\n",
    "        state_index = next_state_index\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode + 1} completed.\")\n",
    "\n",
    "print(\"\\nLearned Q-table:\")\n",
    "print(Q)\n",
    "\n",
    "print(\"\\nLearned Policy (arrows show the best action for each cell):\")\n",
    "env.print_policy(Q)\n",
    "\n",
    "# Testing the learned policy from the start state\n",
    "print(\"\\nTest Run:\")\n",
    "state = env.reset()\n",
    "steps = 0\n",
    "path = [state]\n",
    "done = False\n",
    "while not done and steps < 50:\n",
    "    state_index = env.state_to_index(state)\n",
    "    action = np.argmax(Q[state_index])\n",
    "    state, reward, done = env.step(action)\n",
    "    path.append(state)\n",
    "    steps += 1\n",
    "\n",
    "print(\"Path taken:\", path)\n",
    "if done:\n",
    "    print(f\"Goal reached in {steps} steps!\")\n",
    "else:\n",
    "    print(\"Failed to reach goal within the step limit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c1f8d9-bb82-4aec-a1a8-f23c3b2e52f9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a Q-learning algorithm using a simple maze problem as motivation. The agent learns the optimal policy using the Q-update rule:\n",
    "\n",
    "  Q(s, a) = Q(s, a) + α [r + γ maxₐ′ Q(s′, a′) – Q(s, a)]\n",
    "\n",
    "Experiment with the maze layout, learning parameters, or number of episodes to see how performance is affected. Happy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e543c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-dec-finetuning-dev-gfUsYha8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
